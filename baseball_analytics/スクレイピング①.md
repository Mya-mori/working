# ニュースのワード分析①
### ~スクレイピングからBigQueryに挿入まで~

## きっかけ

　野球ニュースをスクレイピングし, そのニュースで使用されている言葉を分析したいと考ました(阪神ファンであるが, たしか開幕1勝14敗?という奇跡の記録を刻みニュースの分析を行いたいと思った)

## 目次

1. 分析フロー
2. 対象サイトの決定
3. スクレイピング
4. BigQueryでの作業
5. スクレイピングしたデータをテーブルに挿入
6. 課題
7. 次回以降の作業

## 1. 分析フロー

　画像(*1)は大まかな作業内容を示した. 対象となるウェブサイトからスクレイピングを行い, その内容をBigQueryに適宜挿入します. その後BigQueryに格納されているデータを使用して, PythonもしくはBIツールで分析を行います.

![figure1. 作業フロー]

figure1. 作業フロー

## 2. 対象サイトの決定

　分析するウェブサイトは[阪神に関するニュース](https://baseball.yahoo.co.jp/npb/teams/5/)とします(現在3勝16敗です！).

![figure2. 分析ウェブサイト](%E3%82%B9%E3%82%AF%E3%83%AC%E3%82%A4%E3%83%92%E3%82%9A%E3%83%B3%E3%82%AF%E3%82%99%E3%81%8B%2026067/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-04-17_23.09.14.png)

figure2. 分析ウェブサイト

　今回スクレイピングを行うのは, 各ニュースのタイトルになります.  <p class> ... </p>に囲まれている箇所になります. MacならF12を押下することで確認できます.

![figure3. ウェブサイトのhtml構造](%E3%82%B9%E3%82%AF%E3%83%AC%E3%82%A4%E3%83%92%E3%82%9A%E3%83%B3%E3%82%AF%E3%82%99%E3%81%8B%2026067/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-04-17_23.20.17.png)

figure3. ウェブサイトのhtml構造

## 3.  スクレイピング(Python)

　[図解！Python BeautifulSoupの使い方を徹底解説！](https://ai-inter1.com/beautifulsoup_1/)の記事を参考にスクレイピングの作業を進めていきます. Google Colabの使用を想定しています.

1. まずは必要となるライブラリをインポートします.

```python
pip install beautifulsoup4
pip install requests
pip install pandas
```

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
```

1. 今回使用するウェブサイトのurlを取得します. その後, BeautifulSoupの第1引数に対象サイトのurlを, 第2引数に使用するパーサーを選択します. 

```python
url = 'https://baseball.yahoo.co.jp/npb/teams/5/'
res = requests.get(url)

# BeautifulSoup(解析対象のHTML, 利用するパーサー)
soup = BeautifulSoup(res.text, "html.parser")
```

1. htmlタグで指定されている <p class> ... </p>の範囲を指定してスクレイピングを行います. htmlタグの確認はF12で行うことができます. 使用するメソッドはfind_all()メソッドで引数に一致する全ての要素を抽出することができます.  詳細はfind_all()に関する[公式ドキュメント](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)を参照してください. figure2で確認したように引数には”p”, classは”sn-list__itemTitle”を指定します.

```python
import pandas as pd
elems = pd.DataFrame(soup.find_all("p", class_="sn-list__itemTitle"), columns = ["news"])
df = pd.DataFrame(elems["news"], dtype="string")
```

1. またデータを取得した日時も同様に取得したいため”date”カラムを作成し, 格納します.

```python
import datetime
d_today = datetime.date.today()

df["date"] = d_today
df["date"] = pd.to_datetime(df["date"])
```

```python
df.dtypes
--------------
news    string
date    datetime64[ns]
dtype: object
```

## 4.  BigQueryでの作業

　次に取得したデータを格納するためにBigQyery側でテーブルを作成します.

1. 「新しいプロジェクト」を押下, プロジェクト名を入力後「作成」から新規のプロジェクトを作成します

![figure4 新しいプロジェクトの作成](%E3%82%B9%E3%82%AF%E3%83%AC%E3%82%A4%E3%83%92%E3%82%9A%E3%83%B3%E3%82%AF%E3%82%99%E3%81%8B%2026067/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-04-18_10.23.47.png)

figure4 新しいプロジェクトの作成

1. 「プロジェクト名」(figure5ではbigdataSQL)を押下し, 新規作成した「test」オブジェクトを指定します. 

![figure5 プロジェクトの選択](%E3%82%B9%E3%82%AF%E3%83%AC%E3%82%A4%E3%83%92%E3%82%9A%E3%83%B3%E3%82%AF%E3%82%99%E3%81%8B%2026067/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-04-18_10.27.24.png)

figure5 プロジェクトの選択

1. 「エクスプローラー」に新規作成したプロジェクト名が表示されるので, 3点リーダーから「データセットの作成」を押下し, データセットを作成してください.

![スクリーンショット 2022-04-18 10.31.52.png](%E3%82%B9%E3%82%AF%E3%83%AC%E3%82%A4%E3%83%92%E3%82%9A%E3%83%B3%E3%82%AF%E3%82%99%E3%81%8B%2026067/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-04-18_10.31.52.png)

1. データセット作成後, 再度エクスプローラーを確認すると「test」のデータセットを確認できます. 再度３点リーダーから「テーブルを作成」を押下します.

![スクリーンショット 2022-04-18 10.33.14.png](%E3%82%B9%E3%82%AF%E3%83%AC%E3%82%A4%E3%83%92%E3%82%9A%E3%83%B3%E3%82%AF%E3%82%99%E3%81%8B%2026067/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-04-18_10.33.14.png)

1. 「テーブル」でテーブル名を, 「スキーマ」から今回作成した「news」と「date」カラムを入力してください. (注意: dateのカラムタイプはTIMESTAMPを指定してください.  DATETIMEだと型が一致しないというエラーが帰ってきます. 参考: **[Pandas/Google BigQuery: Schema mismatch makes the upload fail](https://stackoverflow.com/questions/44953463/pandas-google-bigquery-schema-mismatch-makes-the-upload-fail),** (機械翻訳)[Pandas/Google bigquery :スキーマの不一致は、アップロードに失敗します](https://jpdebug.com/p/1899002))

![スクリーンショット 2022-04-18 10.50.41.png](%E3%82%B9%E3%82%AF%E3%83%AC%E3%82%A4%E3%83%92%E3%82%9A%E3%83%B3%E3%82%AF%E3%82%99%E3%81%8B%2026067/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-04-18_10.50.41.png)

## 5.  スクレイピングしたデータをテーブルに挿入

1. 再度 Google colabに戻り, データを挿入するためのクエリを書きます.  to_gbqメソッドを使用することで, BigQueryにデータを格納することができます. 第1引数は「データセット.テーブル」, 第2引数は「プロジェクトid」,第3引数は「テーブルが存在する際にどのように処理を行うか」となっており”append”とすることでテーブルにデータを追記するようにします. 

```python
df.to_gbq("test.test", project_id="test-347601", if_exists="append")
```

1. BigQueryに戻り, 「test」テーブルの確認を行います. 以下のように無事データが格納されていれば成功です

![スクリーンショット 2022-04-18 12.44.51.png](%E3%82%B9%E3%82%AF%E3%83%AC%E3%82%A4%E3%83%92%E3%82%9A%E3%83%B3%E3%82%AF%E3%82%99%E3%81%8B%2026067/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88_2022-04-18_12.44.51.png)

## 6.  課題

・Google Colabを定期実行したい

## 7.  次回以降の作業

・作成したテーブルをクレンジングする

・そのテーブルを元に, PythonやBIツールで分析を行う

<参考文献>

1. 図解！Python BeautifulSoupの使い方を徹底解説！

[https://ai-inter1.com/beautifulsoup_1/](https://ai-inter1.com/beautifulsoup_1/)

1. Beautiful Soup Documentation 公式ドキュメント

 [https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

<使用したクエリ>

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd

url = 'https://baseball.yahoo.co.jp/npb/teams/5/'
res = requests.get(url)
soup = BeautifulSoup(res.text, "html.parser")

elems = pd.DataFrame(soup.find_all("p", class_="sn-list__itemTitle"), columns = ["news"])
df = pd.DataFrame(elems["news"], dtype="string")

import datetime
d_today = datetime.date.today()

df["date"] = d_today
df["date"] = pd.to_datetime(df["date"])

df.to_gbq("test.test", project_id="test-347601", if_exists="append")
```